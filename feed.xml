<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ju2ez.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ju2ez.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-09-18T07:42:24+00:00</updated><id>https://ju2ez.github.io/feed.xml</id><title type="html">blogposts</title><subtitle>A little blog for fun :) </subtitle><entry><title type="html">Grokking Einstein Summation for Efficient and Readable Tensor Manipulation</title><link href="https://ju2ez.github.io/blog/2024/einops/" rel="alternate" type="text/html" title="Grokking Einstein Summation for Efficient and Readable Tensor Manipulation"/><published>2024-09-17T14:10:00+00:00</published><updated>2024-09-17T14:10:00+00:00</updated><id>https://ju2ez.github.io/blog/2024/einops</id><content type="html" xml:base="https://ju2ez.github.io/blog/2024/einops/"><![CDATA[<p>Einsum notation stands as an incredibly powerful and intuitive tool, ideal for working with tensor computations. Its universal applicability transcends dependencies on specific libraries such as Pytorch, Jax, or Numpy, underscoring its broad utility.</p> <h1 id="history-of-the-einsteinsum">History of the Einstein Sum</h1> <p>As the name suggests, Einstein summation was first proposed by Einstein. In 1916, he proposed this notation in his famous manuscript “The Foundation of the General Theory of Relativity”.</p> <p>Page 781 from “Die Grundlage der allgeminen Relatitivtätstheorie” - Albert Einstein</p> <p>What Einstein essentially proposed, originally articulated in German - a more common language in science at that time - is that if an index appears twice in a summation, we assume it is summed over, unless stated otherwise. This assumption allows us to omit the summation sign, thereby simplifying the equation. For instance, a simple sum over the first five elements of two vectors \(y = \sum_{i=0}^{5} c_i x^i = c_0 x^0 + c_1 x^1 + c_2 x^2 + c_3 x^3 + c_4 x^4 + c_5 x^5\) Explicit sum over the range of indices of two vectors c and x.can be expressed in simpler terms as \(y = c_i x^i\) Einsum omits the sum sign.Note that by omitting the summation sign and summing over the entire range of both vectors, we implicitly assume that both vectors have the same number of elements, which in this case is five. At this point, you might assume that this simple abbreviation encompasses all there is to know about the Einstein notation. However, you will soon discover that this powerful concept has been significantly extended and applied in various ways, finding its place in numerous renowned mathematical frameworks. Einstein Sum in Modern Tech Frameworks Einsum also found its ways into modern tech frameworks. NumPy introduced it in May 2011( https://lwn.net/Articles/443312/) with the release of 1.6.0. Further, Pytorch introduced it in one of its early version 0.4.0 in mid 2018. And by now it is a universally adopted way to explicitly show how to combine tensors.  Einops - a powerful extension of the Einsum Einops stands for Einstein-Inspired Notation for operations or simply Einstein operations. It is a powerful library that extends the concept of the Einsum by additional frequently used tensor operations. In the following, we will go with Einops, to introduce the most powerful concepts. Einsum Let us start with the composition of tensors using the simple weighted sum that produced our y. Essentially, we can model this as a loop over the two vectors:</p> <pre><code class="language-Python3">y = 0
c = [0, 1, 2, 3, 4, 5]
x = [6, 1, 3, 5, 6, 9]
for i in range(0, 6):
  y += c[i]*x[i]
</code></pre> <p>In PyTorch we could shorten this by for instance using the dot function or the @ operator.</p> <pre><code class="language-Python3"># using the dot function
y = c.dot(x)
# the add operator can be used for the dot product
y = c@x
</code></pre> <p>Now, let’s see how this would look with Einsum. First off, many tensor frameworks come with their own variant of Einsum. Before we compare it to PyTorch’s implementation, let me make a case for Einops: Framework Agnostic: Einops is designed to be agnostic of the underlying framework. This means we can use the same language across different tensor frameworks, allowing us to maintain consistent terminology regardless of the framework in use. Expressiveness: In many cases, tensor frameworks restrict us to using a single letter for each dimension, limiting expressiveness. Einops, on the other hand, allows for more descriptive dimension names, making it easier to explicitly convey what’s happening.</p> <p>Let’s remind ourselves of a fundamental concept from the Zen of Python: “Explicit is better than implicit.”</p> <pre><code class="language-Python3">c = torch.tensor([0, 1, 2, 3, 4, 5])
x = torch.tensor([6, 1, 3, 5, 6, 9])
y = torch.einsum('i,i-&gt;', c, x) # dot product with torch
y = einops.einsum(C, X, "index,index-&gt;") # dot product with einops
</code></pre> <h1 id="note-that-with-torch-we-pass-the-variables-at-the-end-whereas">note that with torch we pass the variables at the end, whereas</h1> <h1 id="in-einops-with-pass-it-in-the-beginnning">in einops with pass it in the beginnning</h1> <p>While this may seem unnecessarily verbose for a simple one-dimensional case, the advantages become clear when we start working with higher-dimensional tensors.</p> <p>Rearrange, Reduce, Repeat - simple Einops functions With just these three fundamental operations, a wide range of tensor manipulations becomes possible. Where in most frameworks, you tipically flip dimensions by reffering to them with their indices, rearrange makes is possible to explicitly transpose dimensions of a tensor giving them names that can be better understood. Assume an image with height h, width w, and color channel c. Now let us assume we would want to flip the image by its height and width.  Typically this would like something like this:</p> <h1 id="example">example</h1> <h1 id="this-is-how-a-simple-transpose-might-look-in-ordinary-math-frameworks">this is how a simple transpose might look in ordinary math frameworks</h1> <h1 id="assuming-an-image-tensor-with-shape-h-w-c">assuming an image tensor with shape (h, w, c)</h1> <p>image.transpose(0, 1) # this operation transposes h and w and you can see how it could become tedious to keep track of which dimension corresponds to which element. With Einops and rearrange we can state the same transpose as follows,</p> <h1 id="example-1">example</h1> <h1 id="we-transpose-height-and-width-of-the-image-using-einops-rearrange">we transpose height and width of the image using Einops rearrange</h1> <h1 id="we-give-a-explict-naming-to-the-dimensions">we give a explict naming to the dimensions</h1> <pre><code class="language-Python3">from einops import rearrange
rearrange(image, "h w c -&gt; w h c")
</code></pre> <p>and by explicitly naming the dimensions of the image, it becomes much easier to keep track of what is happening, thereby reducing the need for comments or debugging. Einops also allows to give longer names, as long as we do not seperate them with spaces. Hence, this statement is equivalent:</p> <pre><code class="language-Python3"># this code
rearrange(image, "height width channel_dim -&gt; width height channel_dim")
# is equivalent to
rearrange(image, "h w c -&gt; w h c")
# and to
image.transpose(0, 1) # this operation transposes h and w
</code></pre> <p>But this is not all there is to</p>]]></content><author><name></name></author><category term="blogpost"/><category term="deep-learning"/><category term="framework"/><category term="einops"/><summary type="html"><![CDATA[The inner workings of the Einops package]]></summary></entry><entry><title type="html">Towards Group Equivariant Self-Attention</title><link href="https://ju2ez.github.io/blog/2024/towards-group-equivariant-self-attention/" rel="alternate" type="text/html" title="Towards Group Equivariant Self-Attention"/><published>2024-01-10T11:59:59+00:00</published><updated>2024-01-10T11:59:59+00:00</updated><id>https://ju2ez.github.io/blog/2024/towards-group-equivariant-self-attention</id><content type="html" xml:base="https://ju2ez.github.io/blog/2024/towards-group-equivariant-self-attention/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Group Equivariant Self-Attention</title><link href="https://ju2ez.github.io/blog/2024/group-equivariant-self-attention/" rel="alternate" type="text/html" title="Group Equivariant Self-Attention"/><published>2024-01-09T23:59:11+00:00</published><updated>2024-01-09T23:59:11+00:00</updated><id>https://ju2ez.github.io/blog/2024/group-equivariant-self-attention</id><content type="html" xml:base="https://ju2ez.github.io/blog/2024/group-equivariant-self-attention/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Injecting geometric priors into the Transformer modelContinue reading on Towards Data Science »]]></summary></entry><entry><title type="html">Stand-Alone Self-Attention in Vision From Scratch</title><link href="https://ju2ez.github.io/blog/2023/stand-alone-self-attention-in-vision-from-scratch/" rel="alternate" type="text/html" title="Stand-Alone Self-Attention in Vision From Scratch"/><published>2023-04-28T06:21:01+00:00</published><updated>2023-04-28T06:21:01+00:00</updated><id>https://ju2ez.github.io/blog/2023/stand-alone-self-attention-in-vision-from-scratch</id><content type="html" xml:base="https://ju2ez.github.io/blog/2023/stand-alone-self-attention-in-vision-from-scratch/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Towards Stand-Alone Self-Attention in Vision</title><link href="https://ju2ez.github.io/blog/2023/towards-stand-alone-self-attention-in-vision/" rel="alternate" type="text/html" title="Towards Stand-Alone Self-Attention in Vision"/><published>2023-04-28T05:56:21+00:00</published><updated>2023-04-28T05:56:21+00:00</updated><id>https://ju2ez.github.io/blog/2023/towards-stand-alone-self-attention-in-vision</id><content type="html" xml:base="https://ju2ez.github.io/blog/2023/towards-stand-alone-self-attention-in-vision/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A deep dive into the application of the transformer architecture and its self-attention operation for visionContinue reading on Towards Data Science »]]></summary></entry><entry><title type="html">The Architecture of Europe’s Gaia-X</title><link href="https://ju2ez.github.io/blog/2021/the-architecture-of-europes-gaia-x/" rel="alternate" type="text/html" title="The Architecture of Europe’s Gaia-X"/><published>2021-06-21T15:27:26+00:00</published><updated>2021-06-21T15:27:26+00:00</updated><id>https://ju2ez.github.io/blog/2021/the-architecture-of-europes-gaia-x</id><content type="html" xml:base="https://ju2ez.github.io/blog/2021/the-architecture-of-europes-gaia-x/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[A technical overviewContinue reading on Towards Data Science »]]></summary></entry><entry><title type="html">Coding Hands-On: Quantum Annealing</title><link href="https://ju2ez.github.io/blog/2021/coding-hands-on-quantum-annealing/" rel="alternate" type="text/html" title="Coding Hands-On: Quantum Annealing"/><published>2021-03-04T13:18:10+00:00</published><updated>2021-03-04T13:18:10+00:00</updated><id>https://ju2ez.github.io/blog/2021/coding-hands-on-quantum-annealing</id><content type="html" xml:base="https://ju2ez.github.io/blog/2021/coding-hands-on-quantum-annealing/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Solving Sudoku with a Quantum Power-Up</title><link href="https://ju2ez.github.io/blog/2021/solving-sudoku-with-a-quantum-power-up/" rel="alternate" type="text/html" title="Solving Sudoku with a Quantum Power-Up"/><published>2021-03-04T13:04:05+00:00</published><updated>2021-03-04T13:04:05+00:00</updated><id>https://ju2ez.github.io/blog/2021/solving-sudoku-with-a-quantum-power-up</id><content type="html" xml:base="https://ju2ez.github.io/blog/2021/solving-sudoku-with-a-quantum-power-up/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Being faster than classical computers is the promise of the mystified Quantum Computer.Continue reading on Towards Data Science »]]></summary></entry></feed>