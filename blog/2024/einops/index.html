<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Grokking Einstein Summation for Efficient and Readable Tensor Manipulation | blogposts </title> <meta name="author" content="Julian "> <meta name="description" content="The inner workings of the Einops package"> <meta name="keywords" content="computer-science, artificial-intelligence, quantum-computing, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ju2ez.github.io/blog/2024/einops/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> blogposts </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/feedback/">feedback </a> </li> <li class="nav-item "> <a class="nav-link" href="/index.html">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Grokking Einstein Summation for Efficient and Readable Tensor Manipulation</h1> <p class="post-meta"> September 17, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/framework"> <i class="fa-solid fa-hashtag fa-sm"></i> framework</a>   <a href="/blog/tag/einops"> <i class="fa-solid fa-hashtag fa-sm"></i> einops</a>     ·   <a href="/blog/category/blogpost"> <i class="fa-solid fa-tag fa-sm"></i> blogpost</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Einsum notation stands as an incredibly powerful and intuitive tool, ideal for working with tensor computations. Its universal applicability transcends dependencies on specific libraries such as Pytorch, Jax, or Numpy, underscoring its broad utility.</p> <h1 id="history-of-the-einsteinsum">History of the Einstein Sum</h1> <p>As the name suggests, Einstein summation was first proposed by Einstein. In 1916, he proposed this notation in his famous manuscript “The Foundation of the General Theory of Relativity”.</p> <p>Page 781 from “Die Grundlage der allgeminen Relatitivtätstheorie” - Albert Einstein</p> <p>What Einstein essentially proposed, originally articulated in German - a more common language in science at that time - is that if an index appears twice in a summation, we assume it is summed over, unless stated otherwise. This assumption allows us to omit the summation sign, thereby simplifying the equation. For instance, a simple sum over the first five elements of two vectors \(y = \sum_{i=0}^{5} c_i x^i = c_0 x^0 + c_1 x^1 + c_2 x^2 + c_3 x^3 + c_4 x^4 + c_5 x^5\) Explicit sum over the range of indices of two vectors c and x.can be expressed in simpler terms as \(y = c_i x^i\) Einsum omits the sum sign.Note that by omitting the summation sign and summing over the entire range of both vectors, we implicitly assume that both vectors have the same number of elements, which in this case is five. At this point, you might assume that this simple abbreviation encompasses all there is to know about the Einstein notation. However, you will soon discover that this powerful concept has been significantly extended and applied in various ways, finding its place in numerous renowned mathematical frameworks. Einstein Sum in Modern Tech Frameworks Einsum also found its ways into modern tech frameworks. NumPy introduced it in May 2011( https://lwn.net/Articles/443312/) with the release of 1.6.0. Further, Pytorch introduced it in one of its early version 0.4.0 in mid 2018. And by now it is a universally adopted way to explicitly show how to combine tensors.  Einops - a powerful extension of the Einsum Einops stands for Einstein-Inspired Notation for operations or simply Einstein operations. It is a powerful library that extends the concept of the Einsum by additional frequently used tensor operations. In the following, we will go with Einops, to introduce the most powerful concepts. Einsum Let us start with the composition of tensors using the simple weighted sum that produced our y. Essentially, we can model this as a loop over the two vectors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">6</span><span class="p">):</span>
  <span class="n">y</span> <span class="o">+=</span> <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</code></pre></div></div> <p>In PyTorch we could shorten this by for instance using the dot function or the @ operator.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># using the dot function
</span><span class="n">y</span> <span class="o">=</span> <span class="n">c</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># the add operator can be used for the dot product
</span><span class="n">y</span> <span class="o">=</span> <span class="n">c</span><span class="nd">@x</span>
</code></pre></div></div> <p>Now, let’s see how this would look with Einsum. First off, many tensor frameworks come with their own variant of Einsum. Before we compare it to PyTorch’s implementation, let me make a case for Einops: Framework Agnostic: Einops is designed to be agnostic of the underlying framework. This means we can use the same language across different tensor frameworks, allowing us to maintain consistent terminology regardless of the framework in use. Expressiveness: In many cases, tensor frameworks restrict us to using a single letter for each dimension, limiting expressiveness. Einops, on the other hand, allows for more descriptive dimension names, making it easier to explicitly convey what’s happening.</p> <p>Let’s remind ourselves of a fundamental concept from the Zen of Python: “Explicit is better than implicit.”</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">i,i-&gt;</span><span class="sh">'</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="c1"># dot product with torch
</span><span class="n">y</span> <span class="o">=</span> <span class="n">einops</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="sh">"</span><span class="s">index,index-&gt;</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># dot product with einops
</span></code></pre></div></div> <h1 id="note-that-with-torch-we-pass-the-variables-at-the-end-whereas">note that with torch we pass the variables at the end, whereas</h1> <h1 id="in-einops-with-pass-it-in-the-beginnning">in einops with pass it in the beginnning</h1> <p>While this may seem unnecessarily verbose for a simple one-dimensional case, the advantages become clear when we start working with higher-dimensional tensors.</p> <p>Rearrange, Reduce, Repeat - simple Einops functions With just these three fundamental operations, a wide range of tensor manipulations becomes possible. Where in most frameworks, you tipically flip dimensions by reffering to them with their indices, rearrange makes is possible to explicitly transpose dimensions of a tensor giving them names that can be better understood. Assume an image with height h, width w, and color channel c. Now let us assume we would want to flip the image by its height and width.  Typically this would like something like this:</p> <h1 id="example">example</h1> <h1 id="this-is-how-a-simple-transpose-might-look-in-ordinary-math-frameworks">this is how a simple transpose might look in ordinary math frameworks</h1> <h1 id="assuming-an-image-tensor-with-shape-h-w-c">assuming an image tensor with shape (h, w, c)</h1> <p>image.transpose(0, 1) # this operation transposes h and w and you can see how it could become tedious to keep track of which dimension corresponds to which element. With Einops and rearrange we can state the same transpose as follows,</p> <h1 id="example-1">example</h1> <h1 id="we-transpose-height-and-width-of-the-image-using-einops-rearrange">we transpose height and width of the image using Einops rearrange</h1> <h1 id="we-give-a-explict-naming-to-the-dimensions">we give a explict naming to the dimensions</h1> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">einops</span> <span class="kn">import</span> <span class="n">rearrange</span>
<span class="nf">rearrange</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="sh">"</span><span class="s">h w c -&gt; w h c</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>and by explicitly naming the dimensions of the image, it becomes much easier to keep track of what is happening, thereby reducing the need for comments or debugging. Einops also allows to give longer names, as long as we do not seperate them with spaces. Hence, this statement is equivalent:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># this code
</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="sh">"</span><span class="s">height width channel_dim -&gt; width height channel_dim</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># is equivalent to
</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="sh">"</span><span class="s">h w c -&gt; w h c</span><span class="sh">"</span><span class="p">)</span>
<span class="c1"># and to
</span><span class="n">image</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># this operation transposes h and w
</span></code></pre></div></div> <p>But this is not all there is to</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/solving-sudoku-with-a-quantum-power-up/">Solving Sudoku with a Quantum Power-Up</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/coding-hands-on-quantum-annealing/">Coding Hands-On: Quantum Annealing</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/the-architecture-of-europes-gaia-x/">The Architecture of Europe’s Gaia-X</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/towards-stand-alone-self-attention-in-vision/">Towards Stand-Alone Self-Attention in Vision</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/stand-alone-self-attention-in-vision-from-scratch/">Stand-Alone Self-Attention in Vision From Scratch</a> </li> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-V6WVPRDVBY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-V6WVPRDVBY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>