---
layout: post
title: Grokking Einstein Summation for Efficient and Readable Tensor Manipulation
date: 2024-09-17 14:10:00
description: The inner workings of the Einops package
tags:
  - deep-learning
  - framework
  - einops
categories: blogpost
published: false
---

Einsum notation stands as an incredibly powerful and intuitive tool, ideal for working with tensor computations. Its universal applicability transcends dependencies on specific libraries such as Pytorch, Jax, or Numpy, underscoring its broad utility.
# History of the Einstein Sum
As the name suggests, Einstein summation was first proposed by Einstein.
In 1916, he proposed this notation in his famous manuscript "The Foundation of the General Theory of Relativity".

Page 781 from "Die Grundlage der allgeminen Relatitivtätstheorie" - Albert Einstein

What Einstein essentially proposed, originally articulated in German - a more common language in science at that time - is that if an index appears twice in a summation, we assume it is summed over, unless stated otherwise. This assumption allows us to omit the summation sign, thereby simplifying the equation.
For instance, a simple sum over the first five elements of two vectors
$$
y = \sum_{i=0}^{5} c_i x^i = c_0 x^0 + c_1 x^1 + c_2 x^2 + c_3 x^3 + c_4 x^4 + c_5 x^5
$$
The explicit sum over the range of indices over c and x can be expressed in simpler terms as
$$
y = c_i x^i
$$

Note that by omitting the summation sign and summing over the entire range of both vectors, we implicitly assume that both vectors have the same number of elements, which in this case is five.
At this point, you might assume that this simple abbreviation encompasses all there is to know about the Einstein notation. However, you will soon discover that this powerful concept has been significantly extended and applied in various ways, finding its place in numerous renowned mathematical frameworks.
Einstein Sum in Modern Tech Frameworks
Einsum also found its ways into modern tech frameworks.
NumPy introduced it in May 2011( https://lwn.net/Articles/443312/) with the release of 1.6.0.
Further, Pytorch introduced it in one of its early version 0.4.0 in mid 2018.
And by now it is a universally adopted way to explicitly show how to combine tensors. 
Einops - a powerful extension of the Einsum
Einops stands for Einstein-Inspired Notation for operations or simply Einstein operations. It is a powerful library that extends the concept of the Einsum by additional frequently used tensor operations. In the following, we will go with Einops, to introduce the most powerful concepts.
Einsum
Let us start with the composition of tensors using the simple weighted sum that produced our y.
Essentially, we can model this as a loop over the two vectors:

```python
y = 0
c = [0, 1, 2, 3, 4, 5]
x = [6, 1, 3, 5, 6, 9]
for i in range(0, 6):
  y += c[i]*x[i]
```
In PyTorch we could shorten this by for instance using the dot function or the @ operator.


```python
# using the dot function
y = c.dot(x)
# the add operator can be used for the dot product
y = c@x
```

Now, let's see how this would look with Einsum.
First off, many tensor frameworks come with their own variant of Einsum. Before we compare it to PyTorch's implementation, let me make a case for Einops:
Framework Agnostic: Einops is designed to be agnostic of the underlying framework. This means we can use the same language across different tensor frameworks, allowing us to maintain consistent terminology regardless of the framework in use.
Expressiveness: In many cases, tensor frameworks restrict us to using a single letter for each dimension, limiting expressiveness. Einops, on the other hand, allows for more descriptive dimension names, making it easier to explicitly convey what's happening.

Let's remind ourselves of a fundamental concept from the Zen of Python: "Explicit is better than implicit."
```python
c = torch.tensor([0, 1, 2, 3, 4, 5])
x = torch.tensor([6, 1, 3, 5, 6, 9])
y = torch.einsum('i,i->', c, x) # dot product with torch
y = einops.einsum(C, X, "index,index->") # dot product with einops
```
# note that with torch we pass the variables at the end, whereas
# in einops with pass it in the beginnning
While this may seem unnecessarily verbose for a simple one-dimensional case, the advantages become clear when we start working with higher-dimensional tensors.

Rearrange, Reduce, Repeat - simple Einops functions
With just these three fundamental operations, a wide range of tensor manipulations becomes possible.
Where in most frameworks, you tipically flip dimensions by reffering to them with their indices, rearrange makes is possible to explicitly transpose dimensions of a tensor giving them names that can be better understood.
Assume an image with height h, width w, and color channel c. Now let us assume we would want to flip the image by its height and width. 
Typically this would like something like this:
# example
# this is how a simple transpose might look in ordinary math frameworks
# assuming an image tensor with shape (h, w, c)
image.transpose(0, 1) # this operation transposes h and w
and you can see how it could become tedious to keep track of which dimension corresponds to which element.
With Einops and rearrange we can state the same transpose as follows,
# example
# we transpose height and width of the image using Einops rearrange
# we give a explict naming to the dimensions

```python
from einops import rearrange
rearrange(image, "h w c -> w h c")
```

and by explicitly naming the dimensions of the image, it becomes much easier to keep track of what is happening, thereby reducing the need for comments or debugging.
Einops also allows to give longer names, as long as we do not seperate them with spaces. Hence, this statement is equivalent:


```python
# this code
rearrange(image, "height width channel_dim -> width height channel_dim")
# is equivalent to
rearrange(image, "h w c -> w h c")
# and to
image.transpose(0, 1) # this operation transposes h and w
```
But this is not all there is to